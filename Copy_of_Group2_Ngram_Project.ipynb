{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9197ee00",
      "metadata": {
        "id": "9197ee00"
      },
      "source": [
        "# Building and Evaluating N-gram Language Models\n",
        "\n",
        "**BSc Data Science â€” CDS 3400: Natural Language Processing**\n",
        "\n",
        "\n",
        "**Contents:**\n",
        "- Theory and motivation\n",
        "- Dataset (Shakespeare Sonnets)\n",
        "- Preprocessing\n",
        "- Building unigram, bigram, trigram models\n",
        "- Add-one smoothing (Laplace)\n",
        "- Interpolation\n",
        "- Text generation\n",
        "- Perplexity evaluation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775c3526",
      "metadata": {
        "id": "775c3526"
      },
      "source": [
        "## Notebook instructions\n",
        "\n",
        "This notebook is designed to be run top-to-bottom. It uses **NLTK + custom Python code** (no advanced deep-learning libraries). The notebook will:\n",
        "\n",
        "1. Download the Shakespeare sonnets from Project Gutenberg.\n",
        "2. Preprocess the text and split into train/test sets.\n",
        "3. Build counts for unigram, bigram, and trigram models.\n",
        "4. Implement add-one smoothing and an interpolation model.\n",
        "5. Generate \"Shakespeare-like\" lines.\n",
        "6. Calculate perplexity on a held-out test set.\n",
        "\n",
        " `shakespeare_sonnets.txt`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f015aee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f015aee",
        "outputId": "15c98201-cb66-411b-e8b6-3636c7bfd81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports ready. NLTK packages attempted (downloaded if missing).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports and basic helpers\n",
        "import re, os, random, math\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Ensure required NLTK packages are available (these lines will attempt to download if missing)\n",
        "nltk_packages = ['punkt', 'averaged_perceptron_tagger', 'stopwords']\n",
        "for pkg in nltk_packages:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{pkg}')\n",
        "    except LookupError:\n",
        "        try:\n",
        "            nltk.download(pkg)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"Imports ready. NLTK packages attempted (downloaded if missing).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07d3ccf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b07d3ccf",
        "outputId": "0696c404-ceda-45dc-9062-4d3dd0090fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download sonnets from Project Gutenberg...\n",
            "Downloaded and saved to shakespeare_sonnets.txt.\n",
            "Loaded text length: 96312\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Load Shakespeare sonnets text\n",
        "# Primary attempt: download from Project Gutenberg\n",
        "url = \"https://www.gutenberg.org/files/1041/1041-0.txt\"\n",
        "local_filename = \"shakespeare_sonnets.txt\"\n",
        "\n",
        "def download_sonnets(url=url, local_filename=local_filename):\n",
        "    try:\n",
        "        import requests\n",
        "        print(\"Attempting to download sonnets from Project Gutenberg...\")\n",
        "        r = requests.get(url, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        text = r.text\n",
        "        with open(local_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "        print(f\"Downloaded and saved to {local_filename}.\")\n",
        "        return local_filename\n",
        "    except Exception as e:\n",
        "        print(\"Automatic download failed:\", e)\n",
        "        if os.path.exists(local_filename):\n",
        "            print(f\"Found local file {local_filename}. Using it.\")\n",
        "            return local_filename\n",
        "        else:\n",
        "            print(\"No local copy found. Please download the sonnets from Project Gutenberg and save as 'shakespeare_sonnets.txt' in the working directory.\")\n",
        "            raise\n",
        "\n",
        "try:\n",
        "    data_path = download_sonnets()\n",
        "except Exception as exc:\n",
        "    # If running in an environment without internet, raise to show the user what to do.\n",
        "    raise RuntimeError(\"Could not download or locate the sonnets file. Please place 'shakespeare_sonnets.txt' in the working directory.\") from exc\n",
        "\n",
        "# Read the file\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Loaded text length:\", len(raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d99540",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81d99540",
        "outputId": "2538b52f-071b-4188-81c1-690820ddc722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 76 sentences (raw heuristic).\n",
            "1 - the sonnets nby william shakespeare nfrom fairest creatures we desire increase nthat thereby beauty s rose might never die nbut as the riper should by time decease nhis tender heir might bear his memory nbut thou contracted to thine own bright eyes nfeed st thy light s flame with self substantial fuel nmaking a famine where abundance lies nthyself thy foe to thy sweet self too cruel nthou that art now the world s fresh ornament nand only herald to the gaudy spring nwithin thine own bud buriest thy content nand tender churl mak st waste in niggarding npity the world or else this glutton be nto eat the world s due by the grave and thee nii nwhen forty winters shall besiege thy brow nand dig deep trenches in thy beauty s field nthy youth s proud livery so gazed on now nwill be a tatter d weed of small worth held nthen being asked where all thy beauty lies nwhere all the treasure of thy lusty days nto say within thine own deep sunken eyes nwere an all eating shame and thriftless praise nhow much more praise deserv d thy beauty s use nif thou couldst answer this fair child of mine nshall sum my count and make my old excuse nproving his beauty by succession thine nthis were to be new made when thou art old nand see thy blood warm when thou feel st it cold niii nlook in thy glass and tell the face thou viewest nnow is the time that face should form another nwhose fresh repair if now thou not renewest nthou dost beguile the world unbless some mother nfor where is she so fair whose unear d womb ndisdains the tillage of thy husbandry nor who is he so fond will be the tomb nof his self love to stop posterity nthou art thy mother s glass and she in thee ncalls back the lovely april of her prime nso thou through windows of thine age shalt see ndespite of wrinkles this thy golden time nbut if thou live remember d not to be ndie single and thine image dies with thee niv nunthrifty loveliness why dost thou spend nupon thyself thy beauty s legacy nnature s bequest gives nothing but doth lend nand being frank she lends to those are free nthen beauteous niggard why dost thou abuse nthe bounteous largess given thee to give nprofitless usurer why dost thou use nso great a sum of sums yet canst not live nfor having traffic with thyself alone nthou of thyself thy sweet self dost deceive nthen how when nature calls thee to be gone nwhat acceptable audit canst thou leave nthy unused beauty must be tombed with thee nwhich used lives th executor to be nthose hours that with gentle work did frame nthe lovely gaze where every eye doth dwell nwill play the tyrants to the very same nand that unfair which fairly doth excel nfor never resting time leads summer on nto hideous winter and confounds him there nsap checked with frost and lusty leaves quite gone nbeauty o er snowed and bareness every where nthen were not summer s distillation left na liquid prisoner pent in walls of glass nbeauty s effect with beauty were bereft nnor it nor no remembrance what it was nbut flowers distill d though they with winter meet nleese but their show their substance still lives sweet nvi nthen let not winter s ragged hand deface nin thee thy summer ere thou be distill d nmake sweet some vial treasure thou some place nwith beauty s treasure ere it be self kill d nthat use is not forbidden usury nwhich happies those that pay the willing loan nthat s for thyself to breed another thee nor ten times happier be it ten for one nten times thyself were happier than thou art nif ten of thine ten times refigur d thee nthen what could death do if thou shouldst depart nleaving thee living in posterity nbe not self will d for thou art much too fair nto be death s conquest and make worms thine heir nvii nlo\n",
            "2 - in the orient when the gracious light nlifts up his burning head each under eye ndoth homage to his new appearing sight nserving with looks his sacred majesty nand having climb d the steep up heavenly hill nresembling strong youth in his middle age nyet mortal looks adore his beauty still nattending on his golden pilgrimage nbut when from highmost pitch with weary car nlike feeble age he reeleth from the day nthe eyes fore duteous now converted are nfrom his low tract and look another way nso thou thyself outgoing in thy noon nunlook d on diest unless thou get a son nviii nmusic to hear why hear st thou music sadly nsweets with sweets war not joy delights in joy nwhy lov st thou that which thou receiv st not gladly nor else receiv st with pleasure thine annoy nif the true concord of well tuned sounds nby unions married do offend thine ear nthey do but sweetly chide thee who confounds nin singleness the parts that thou shouldst bear nmark how one string sweet husband to another nstrikes each in each by mutual ordering nresembling sire and child and happy mother nwho all in one one pleasing note do sing nwhose speechless song being many seeming one nsings this to thee thou single wilt prove none nix nis it for fear to wet a widow s eye nthat thou consum st thyself in single life nah\n",
            "3 - if thou issueless shalt hap to die nthe world will wail thee like a makeless wife nthe world will be thy widow and still weep nthat thou no form of thee hast left behind nwhen every private widow well may keep nby children s eyes her husband s shape in mind nlook\n",
            "4 - what an unthrift in the world doth spend nshifts but his place for still the world enjoys it nbut beauty s waste hath in the world an end nand kept unused the user so destroys it nno love toward others in that bosom sits nthat on himself such murd rous shame commits nfor shame\n",
            "5 - deny that thou bear st love to any nwho for thyself art so unprovident ngrant if thou wilt thou art belov d of many nbut that thou none lov st is most evident nfor thou art so possess d with murderous hate nthat gainst thyself thou stick st not to conspire nseeking that beauteous roof to ruinate nwhich to repair should be thy chief desire no\n",
            "6 - change thy thought that i may change my mind nshall hate be fairer lodg d than gentle love nbe as thy presence is gracious and kind nor to thyself at least kind hearted prove nmake thee another self for love of me nthat beauty still may live in thine or thee nxi nas fast as thou shalt wane so fast thou grow st nin one of thine from that which thou departest nand that fresh blood which youngly thou bestow st nthou mayst call thine when thou from youth convertest nherein lives wisdom beauty and increase nwithout this folly age and cold decay nif all were minded so the times should cease nand threescore year would make the world away nlet those whom nature hath not made for store nharsh featureless and rude barrenly perish nlook whom she best endow d she gave thee more nwhich bounteous gift thou shouldst in bounty cherish nshe carv d thee for her seal and meant thereby nthou shouldst print more not let that copy die nxii nwhen i do count the clock that tells the time nand see the brave day sunk in hideous night nwhen i behold the violet past prime nand sable curls all silvered o er with white nwhen lofty trees i see barren of leaves nwhich erst from heat did canopy the herd nand summer s green all girded up in sheaves nborne on the bier with white and bristly beard nthen of thy beauty do i question make nthat thou among the wastes of time must go nsince sweets and beauties do themselves forsake nand die as fast as they see others grow nand nothing gainst time s scythe can make defence nsave breed to brave him when he takes thee hence nxiii no\n"
          ]
        }
      ],
      "source": [
        "# 2) Preprocessing: isolate sonnets and clean text\n",
        "# Project Gutenberg sonnets file contains copyright headers. We'll try to extract the sonnets section heuristically.\n",
        "\n",
        "def extract_sonnets(text):\n",
        "    # Heuristic: split by \"THE SONNETS\" or by \"SHAKESPEARE\" headings, otherwise just use whole text\n",
        "    low = text.lower()\n",
        "    if \"sonnets\" in low:\n",
        "        # attempt to find the start of the sonnets section\n",
        "        m = re.search(r\"(?:sonnets)|(the sonnets)\", low)\n",
        "        start = m.start() if m else 0\n",
        "        excerpt = text[start:]\n",
        "    else:\n",
        "        excerpt = text\n",
        "    # Split into lines and keep those that look like sonnet lines (skip Gutenberg headers/footers)\n",
        "    lines = [ln.strip() for ln in excerpt.splitlines() if ln.strip()]\n",
        "    # Attempt to find lines that include Roman numerals (sonnet numbers) or \"Sonnet\" words, otherwise fallback\n",
        "    # For safety, we'll use a simple filter to remove Gutenberg header/footer lines\n",
        "    filtered = [ln for ln in lines if len(ln) > 1 and not ln.lower().startswith('project gutenberg') and 'gutenberg' not in ln.lower()]\n",
        "    return \"\\\\n\".join(filtered)\n",
        "\n",
        "sonnets_text = extract_sonnets(raw_text)\n",
        "\n",
        "# Basic cleanup and sentence splitting\n",
        "# We'll use a simple tokenizer: split into sentences via NLTK, then into word tokens.\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(sonnets_text)\n",
        "print(f\"Extracted {len(sentences)} sentences (raw heuristic).\")\n",
        "\n",
        "# Lowercase and remove non-word characters except apostrophes and spaces\n",
        "def clean_text(s):\n",
        "    s = s.lower()\n",
        "    # keep letters, numbers, apostrophes and spaces\n",
        "    s = re.sub(r\"[^a-z0-9'\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "clean_sentences = [clean_text(s) for s in sentences if len(clean_text(s).split()) >= 1]\n",
        "\n",
        "# Show a sample\n",
        "for i, s in enumerate(clean_sentences[:6]):\n",
        "    print(i+1, \"-\", s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f45e65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4f45e65",
        "outputId": "23b01cce-8601-48ed-ab0f-6f9ffa38b13d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sentences: 60, Test sentences: 16\n",
            "Vocabulary size (train): 3034\n",
            "Top 10 unigrams: [('my', 279), ('the', 273), ('to', 266), ('of', 265), ('i', 254), ('in', 240), ('thy', 218), ('and', 204), ('that', 199), ('nand', 192)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3) Train-test split and build counts for unigrams, bigrams, trigrams\n",
        "random.seed(42)\n",
        "sentences_shuffled = clean_sentences.copy()\n",
        "random.shuffle(sentences_shuffled)\n",
        "\n",
        "# Use ~80% train, 20% test (by sentences)\n",
        "split_idx = int(0.8 * len(sentences_shuffled))\n",
        "train_sents = sentences_shuffled[:split_idx]\n",
        "test_sents = sentences_shuffled[split_idx:]\n",
        "\n",
        "print(f\"Train sentences: {len(train_sents)}, Test sentences: {len(test_sents)}\")\n",
        "\n",
        "def tokenize_sentence(s):\n",
        "    return s.split()  # already cleaned and lowercased\n",
        "\n",
        "# Build counts\n",
        "unigram_counts = Counter()\n",
        "bigram_counts = Counter()\n",
        "trigram_counts = Counter()\n",
        "vocab = set()\n",
        "\n",
        "for sent in train_sents:\n",
        "    tokens = tokenize_sentence(sent)\n",
        "    vocab.update(tokens)\n",
        "    # add start tokens for context handling\n",
        "    tokens_padded = [\"<s>\"] + tokens + [\"</s>\"]\n",
        "    for i in range(len(tokens_padded)):\n",
        "        if i >= 1:\n",
        "            unigram_counts[tokens_padded[i]] += 1\n",
        "        if i >= 2:\n",
        "            bigram = (tokens_padded[i-1], tokens_padded[i])\n",
        "            bigram_counts[bigram] += 1\n",
        "        if i >= 3:\n",
        "            trigram = (tokens_padded[i-2], tokens_padded[i-1], tokens_padded[i])\n",
        "            trigram_counts[trigram] += 1\n",
        "\n",
        "V = len(vocab)\n",
        "print(f\"Vocabulary size (train): {V}\")\n",
        "print(\"Top 10 unigrams:\", unigram_counts.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232c7d0d",
      "metadata": {
        "id": "232c7d0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 4) Probability estimators: MLE, Add-one smoothing, Interpolation\n",
        "def mle_unigram(w):\n",
        "    total = sum(unigram_counts.values())\n",
        "    return unigram_counts[w] / total if total > 0 else 0.0\n",
        "\n",
        "def mle_bigram(w_prev, w):\n",
        "    denom = unigram_counts[w_prev] if unigram_counts[w_prev] > 0 else 0\n",
        "    num = bigram_counts[(w_prev, w)]\n",
        "    return num / denom if denom > 0 else 0.0\n",
        "\n",
        "def mle_trigram(w_prev2, w_prev1, w):\n",
        "    denom = bigram_counts[(w_prev2, w_prev1)] if bigram_counts[(w_prev2, w_prev1)] > 0 else 0\n",
        "    num = trigram_counts[(w_prev2, w_prev1, w)]\n",
        "    return num / denom if denom > 0 else 0.0\n",
        "\n",
        "# Add-one (Laplace) smoothed probabilities\n",
        "def add1_bigram(w_prev, w, V=V):\n",
        "    num = bigram_counts[(w_prev, w)] + 1\n",
        "    denom = unigram_counts[w_prev] + V\n",
        "    return num / denom\n",
        "\n",
        "def add1_trigram(w_prev2, w_prev1, w, V=V):\n",
        "    num = trigram_counts[(w_prev2, w_prev1, w)] + 1\n",
        "    denom = bigram_counts[(w_prev2, w_prev1)] + V\n",
        "    return num / denom\n",
        "\n",
        "# Interpolation (combine unigram, bigram, trigram)\n",
        "lambda1, lambda2, lambda3 = 0.2, 0.3, 0.5  # example weights\n",
        "def interp_prob(w_prev2, w_prev1, w):\n",
        "    p1 = mle_unigram(w)\n",
        "    p2 = mle_bigram(w_prev1, w)\n",
        "    p3 = mle_trigram(w_prev2, w_prev1, w)\n",
        "    return lambda1*p1 + lambda2*p2 + lambda3*p3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f834f75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f834f75",
        "outputId": "21efcfaf-a85c-4cba-87d7-ab40bcf6ae64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample generation (add-one smoothing):\n",
            "shall i favour issue ey get violet bears receives precious here ndarkening ntired aloft supposing live belongs wherever came verses forgoing prophetic\n",
            "\n",
            "Sample generation (interpolation):\n",
            "shall i say nwhat merit lived for gilded tomb nand that which flies before her face dull bearer when from highmost pitch\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5) Text generation using smoothed probabilities (add-one) and interpolation\n",
        "import bisect\n",
        "\n",
        "def sample_from_distribution(dist):\n",
        "    # dist: list of (token, prob) where probs sum to >0\n",
        "    tokens, probs = zip(*dist)\n",
        "    total = sum(probs)\n",
        "    if total <= 0:\n",
        "        return random.choice(tokens)\n",
        "    probs_norm = [p/total for p in probs]\n",
        "    # cumulative\n",
        "    cum = []\n",
        "    s = 0.0\n",
        "    for p in probs_norm:\n",
        "        s += p\n",
        "        cum.append(s)\n",
        "    r = random.random()\n",
        "    idx = bisect.bisect_left(cum, r)\n",
        "    return tokens[min(idx, len(tokens)-1)]\n",
        "\n",
        "def generate_line_add1(start_words=None, max_len=20):\n",
        "    if start_words is None:\n",
        "        w_prev2, w_prev1 = \"<s>\", \"<s>\"\n",
        "    else:\n",
        "        toks = start_words.split()\n",
        "        if len(toks) == 1:\n",
        "            w_prev2, w_prev1 = \"<s>\", toks[-1]\n",
        "        else:\n",
        "            w_prev2, w_prev1 = toks[-2], toks[-1]\n",
        "    out = [] if start_words is None else toks.copy()\n",
        "    for _ in range(max_len):\n",
        "        # build distribution over vocabulary for next word using add-one trigram\n",
        "        dist = []\n",
        "        for w in list(vocab) + [\"</s>\"]:\n",
        "            p = add1_trigram(w_prev2, w_prev1, w)\n",
        "            dist.append((w, p))\n",
        "        nxt = sample_from_distribution(dist)\n",
        "        if nxt == \"</s>\":\n",
        "            break\n",
        "        out.append(nxt)\n",
        "        w_prev2, w_prev1 = w_prev1, nxt\n",
        "    return \" \".join(out)\n",
        "\n",
        "def generate_line_interp(start_words=None, max_len=20):\n",
        "    if start_words is None:\n",
        "        w_prev2, w_prev1 = \"<s>\", \"<s>\"\n",
        "    else:\n",
        "        toks = start_words.split()\n",
        "        if len(toks) == 1:\n",
        "            w_prev2, w_prev1 = \"<s>\", toks[-1]\n",
        "        else:\n",
        "            w_prev2, w_prev1 = toks[-2], toks[-1]\n",
        "    out = [] if start_words is None else toks.copy()\n",
        "    for _ in range(max_len):\n",
        "        dist = []\n",
        "        for w in list(vocab) + [\"</s>\"]:\n",
        "            p = interp_prob(w_prev2, w_prev1, w)\n",
        "            dist.append((w, p))\n",
        "        nxt = sample_from_distribution(dist)\n",
        "        if nxt == \"</s>\":\n",
        "            break\n",
        "        out.append(nxt)\n",
        "        w_prev2, w_prev1 = w_prev1, nxt\n",
        "    return \" \".join(out)\n",
        "\n",
        "# Examples\n",
        "print(\"Sample generation (add-one smoothing):\")\n",
        "print(generate_line_add1(\"shall i\"))\n",
        "print(\"\\nSample generation (interpolation):\")\n",
        "print(generate_line_interp(\"shall i\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d8208d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d8208d",
        "outputId": "0742e181-bdc0-4279-ec7b-b9ec0edc6cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity (Add-One trigram) on test sample: 2877.61\n",
            "Perplexity (Interpolation) on test sample: 17706.88\n"
          ]
        }
      ],
      "source": [
        "# 6) Perplexity calculation for add-one smoothed trigram and for interpolation\n",
        "def perplexity_sentences_add1(sent_list):\n",
        "    log_prob_sum = 0.0\n",
        "    N = 0\n",
        "    for s in sent_list:\n",
        "        tokens = [\"<s>\"] + s.split() + [\"</s>\"]\n",
        "        N += len(tokens) - 1\n",
        "        for i in range(2, len(tokens)):\n",
        "            w_prev2, w_prev1, w = tokens[i-2], tokens[i-1], tokens[i]\n",
        "            p = add1_trigram(w_prev2, w_prev1, w)\n",
        "            log_prob_sum += math.log2(p)\n",
        "    ppl = 2 ** ( - (1.0 / N) * log_prob_sum )\n",
        "    return ppl\n",
        "\n",
        "def perplexity_sentences_interp(sent_list):\n",
        "    log_prob_sum = 0.0\n",
        "    N = 0\n",
        "    for s in sent_list:\n",
        "        tokens = [\"<s>\"] + s.split() + [\"</s>\"]\n",
        "        N += len(tokens) - 1\n",
        "        for i in range(2, len(tokens)):\n",
        "            w_prev2, w_prev1, w = tokens[i-2], tokens[i-1], tokens[i]\n",
        "            p = interp_prob(w_prev2, w_prev1, w)\n",
        "            # smoothing for zero probs (very small floor)\n",
        "            if p <= 0:\n",
        "                p = 1e-12\n",
        "            log_prob_sum += math.log2(p)\n",
        "    ppl = 2 ** ( - (1.0 / N) * log_prob_sum )\n",
        "    return ppl\n",
        "\n",
        "# Compute perplexities on test set\n",
        "ppl_add1 = perplexity_sentences_add1(test_sents[:200])  # limit to first 200 for speed\n",
        "ppl_interp = perplexity_sentences_interp(test_sents[:200])\n",
        "\n",
        "print(f\"Perplexity (Add-One trigram) on test sample: {ppl_add1:.2f}\")\n",
        "print(f\"Perplexity (Interpolation) on test sample: {ppl_interp:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc29b10c",
      "metadata": {
        "id": "cc29b10c"
      },
      "source": [
        "\n",
        "## Results & Discussion\n",
        "\n",
        "- The notebook reports sample generated lines from both the **Add-One (Laplace) smoothed trigram model** and the **Interpolation model**.  \n",
        "- It also computes **perplexity** on a held-out portion of the sonnets (sampled test set).  \n",
        "- Typical observations to include in your presentation:\n",
        "  - **Perplexity** tends to decrease (improve) when interpolation weights favor higher-order models but only if data supports those higher-order n-grams.\n",
        "  - **Add-One smoothing** avoids zero probabilities but can over-smooth, especially with small datasets â€” leading to higher perplexity than more tailored smoothing methods.\n",
        "  - **Generated text** from n-gram models can be locally coherent but lacks long-range structure; it will sound \"Shakespeare-like\" in word choice but not truly Shakespearean in meaning or meter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§© Extension: Implementing Kneserâ€“Ney Smoothing (Advanced Section)\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"Implementing Kneserâ€“Ney smoothing for bigram model...\")\n",
        "\n",
        "# Step 1: Prepare bigram and unigram counts\n",
        "bigram_total = sum(bigram_counts.values())\n",
        "discount = 0.75  # typical discount value\n",
        "\n",
        "# Step 2: Continuation probabilities\n",
        "# P_cont(w_i) = number of unique predecessors of w_i / total number of unique bigrams\n",
        "continuation_counts = defaultdict(int)\n",
        "for (w_prev, w_curr) in bigram_counts.keys():\n",
        "    continuation_counts[w_curr] += 1\n",
        "total_unique_bigrams = len(bigram_counts)\n",
        "P_cont = {w: continuation_counts[w] / total_unique_bigrams for w in vocab}\n",
        "\n",
        "# Step 3: Compute lambda for each w_{i-1}\n",
        "# Î»(w_prev) = (D / c(w_prev)) * (# of unique bigrams starting with w_prev)\n",
        "unique_followers = defaultdict(set)\n",
        "for (w_prev, w_curr) in bigram_counts.keys():\n",
        "    unique_followers[w_prev].add(w_curr)\n",
        "\n",
        "lambda_weight = {}\n",
        "for w_prev in vocab:\n",
        "    if unigram_counts[w_prev] > 0:\n",
        "        lambda_weight[w_prev] = (discount / unigram_counts[w_prev]) * len(unique_followers[w_prev])\n",
        "    else:\n",
        "        lambda_weight[w_prev] = 0.0\n",
        "\n",
        "# Step 4: Define Kneser-Ney probability function\n",
        "def kneser_ney_prob(w_prev, w):\n",
        "    c_bigram = bigram_counts[(w_prev, w)]\n",
        "    c_prev = unigram_counts[w_prev]\n",
        "    term1 = max(c_bigram - discount, 0) / c_prev if c_prev > 0 else 0\n",
        "    term2 = lambda_weight.get(w_prev, 0) * P_cont.get(w, 0)\n",
        "    return term1 + term2\n",
        "\n",
        "# Step 5: Generate text using Kneserâ€“Ney smoothed model\n",
        "def generate_kn_line(start_word=\"<s>\", max_len=15):\n",
        "    prev = start_word\n",
        "    output = [] if start_word == \"<s>\" else [start_word]\n",
        "    for _ in range(max_len):\n",
        "        dist = [(w, kneser_ney_prob(prev, w)) for w in vocab]\n",
        "        total_prob = sum(p for _, p in dist)\n",
        "        if total_prob == 0:\n",
        "            break\n",
        "        probs = [p / total_prob for _, p in dist]\n",
        "        tokens = [w for w, _ in dist]\n",
        "        next_word = random.choices(tokens, weights=probs, k=1)[0]\n",
        "        if next_word == \"</s>\":\n",
        "            break\n",
        "        output.append(next_word)\n",
        "        prev = next_word\n",
        "    return \" \".join(output)\n",
        "\n",
        "# Example: Generate Shakespeare-like line using Kneserâ€“Ney\n",
        "print(\"\\\\nExample Kneserâ€“Ney generation:\")\n",
        "print(generate_kn_line(\"shall\"))\n",
        "\n",
        "# Step 6: Compare a few probabilities\n",
        "sample_pairs = [(\"love\", \"thee\"), (\"i\", \"am\"), (\"summer\", \"day\")]\n",
        "print(\"\\\\nComparison of probabilities (Add-One vs Kneserâ€“Ney):\")\n",
        "for a, b in sample_pairs:\n",
        "    p_add1 = add1_bigram(a, b)\n",
        "    p_kn = kneser_ney_prob(a, b)\n",
        "    print(f\"P({b}|{a}) -> Add-One: {p_add1:.6f}, Kneserâ€“Ney: {p_kn:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kthuBq-eqZ7-",
        "outputId": "bccb25c5-e8b0-46c7-db3e-670c50e2cbad"
      },
      "id": "kthuBq-eqZ7-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementing Kneserâ€“Ney smoothing for bigram model...\n",
            "\\nExample Kneserâ€“Ney generation:\n",
            "shall not the mind being nto of thee all should nthen in odour thence thou nor\n",
            "\\nComparison of probabilities (Add-One vs Kneserâ€“Ney):\n",
            "P(thee|love) -> Add-One: 0.002206, Kneserâ€“Ney: 0.040341\n",
            "P(am|i) -> Add-One: 0.006083, Kneserâ€“Ney: 0.072169\n",
            "P(day|summer) -> Add-One: 0.000328, Kneserâ€“Ney: 0.000361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion: Kneserâ€“Ney Smoothing\n",
        "\n",
        "- **Purpose:** Kneserâ€“Ney is an advanced smoothing method that improves on Add-One by redistributing probability mass based on word continuation frequency.\n",
        "- **Key difference:** Instead of just counting occurrences, it also considers how many *different contexts* a word appears in.\n",
        "- **Benefit:** Produces more realistic probabilities for rare word pairs, especially in smaller corpora.\n",
        "- **Result:** The generated text tends to sound smoother, with less repetition."
      ],
      "metadata": {
        "id": "00qmxu37qiNo"
      },
      "id": "00qmxu37qiNo"
    },
    {
      "cell_type": "markdown",
      "id": "64731f17",
      "metadata": {
        "id": "64731f17"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook implements end-to-end using **NLTK + custom code**. It is intentionally readable and contains markdown explanations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77d0a6f",
      "metadata": {
        "id": "c77d0a6f"
      },
      "source": [
        "\n",
        "## References\n",
        "\n",
        "- *Natural Language Processing in Action* â€” Lane, Howard, Hapke (2019)\n",
        "- Project Gutenberg â€” Shakespeare Sonnets (used as dataset)\n",
        "- NLTK documentation\n",
        "- Lecture notes and online resources on n-gram modeling and smoothing\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}