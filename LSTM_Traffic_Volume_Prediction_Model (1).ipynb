{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2GxFjrkLbgB"
      },
      "source": [
        "## Phase 1A â€” Project Setup & Data (Kaggle â†’ /content)\n",
        "\n",
        "In this step, we:\n",
        "1. Configure the Kaggle API in Google Colab using `kaggle.json`.\n",
        "2. Download the I-94 traffic dataset **once** using the Kaggle API.\n",
        "3. Extract the ZIP file into `/content/` (not Google Drive) for faster reads.\n",
        "4. Load the CSV directly from `/content/...csv` and display basic info.\n",
        "\n",
        "**Output:** Dataset successfully loaded into a pandas DataFrame (`df`) from `/content/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC7eLyfHLabM"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# PHASE 1A: PROJECT SETUP & DATA (KAGGLE)\n",
        "# =========================================\n",
        "\n",
        "# 1) Install Kaggle (usually already installed in Colab, but safe)\n",
        "!pip -q install kaggle\n",
        "\n",
        "# 2) Upload your kaggle.json when prompted (from Kaggle > Account > Create New API Token)\n",
        "from google.colab import files\n",
        "files.upload()  # upload kaggle.json\n",
        "\n",
        "# 3) Put kaggle.json in the right place + permissions\n",
        "!mkdir -p /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# 4) Download the dataset (set to YOUR Kaggle dataset slug)\n",
        "# Example slug for I-94 dataset is commonly: \"fedesoriano/traffic-prediction-dataset\"\n",
        "DATASET_SLUG = \"fedesoriano/traffic-prediction-dataset\"\n",
        "\n",
        "!kaggle datasets download -d {DATASET_SLUG} -p /content --force\n",
        "\n",
        "# 5) Unzip into /content\n",
        "import zipfile, os, glob\n",
        "\n",
        "zip_path = glob.glob(\"/content/*.zip\")[0]\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    z.extractall(\"/content\")\n",
        "\n",
        "print(\"Extracted files:\", os.listdir(\"/content\"))\n",
        "\n",
        "# 6) Locate the CSV and load it from /content (FAST)\n",
        "import pandas as pd\n",
        "\n",
        "csv_files = glob.glob(\"/content/*.csv\")\n",
        "print(\"CSV files found:\", csv_files)\n",
        "\n",
        "# pick the first CSV (weâ€™ll confirm name once it prints)\n",
        "csv_path = csv_files[0]\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"Loaded:\", csv_path)\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldhUOY_MMZt3"
      },
      "source": [
        "## Phase 1B â€” Data Cleaning & Baseline Understanding\n",
        "\n",
        "In this phase, we prepared the I-94 traffic dataset for time-series modeling.\n",
        "\n",
        "### Key Steps\n",
        "- Converted `DateTime` into a proper datetime format.\n",
        "- Sorted data chronologically (mandatory for LSTM models).\n",
        "- Checked for missing values and duplicates.\n",
        "- Extracted basic temporal features (hour, day, month, weekday).\n",
        "- Performed baseline statistical analysis on traffic volume.\n",
        "- Visualized traffic trends over time and hourly patterns.\n",
        "\n",
        "### Observations\n",
        "- Dataset contains **48,120 records** with no missing values.\n",
        "- Traffic volume shows strong temporal patterns.\n",
        "- Clear hourly variation confirms suitability for time-series forecasting.\n",
        "- The `Vehicles` column is selected as the prediction target.\n",
        "\n",
        "**Output:** Cleaned, time-ordered dataset ready for LSTM sequence generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c0XghD3MbI2"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# PHASE 1B: DATA CLEANING & BASELINE EDA\n",
        "# =========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Re-load dataset explicitly (safe practice)\n",
        "df = pd.read_csv(\"/content/traffic.csv\")\n",
        "\n",
        "# 2) Convert DateTime to proper datetime format\n",
        "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
        "\n",
        "# 3) Sort by time (CRITICAL for time-series & LSTM)\n",
        "df = df.sort_values('DateTime').reset_index(drop=True)\n",
        "\n",
        "# 4) Basic structure check\n",
        "print(\"Dataset Info:\")\n",
        "df.info()\n",
        "\n",
        "# 5) Missing values check\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 6) Duplicate check\n",
        "print(\"\\nDuplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "# 7) Basic statistics for target variable\n",
        "print(\"\\nVehicles statistics:\")\n",
        "print(df['Vehicles'].describe())\n",
        "\n",
        "# 8) Time-based feature extraction (for later use)\n",
        "df['hour'] = df['DateTime'].dt.hour\n",
        "df['day'] = df['DateTime'].dt.day\n",
        "df['month'] = df['DateTime'].dt.month\n",
        "df['dayofweek'] = df['DateTime'].dt.dayofweek\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX4krN5YNBp_"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# BASELINE TRAFFIC VOLUME TREND\n",
        "# =========================================\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df['DateTime'], df['Vehicles'])\n",
        "plt.title(\"Traffic Volume Over Time (Baseline)\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Number of Vehicles\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bg7bjoJNLUl"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# HOURLY TRAFFIC PATTERN\n",
        "# =========================================\n",
        "\n",
        "hourly_avg = df.groupby('hour')['Vehicles'].mean()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(hourly_avg.index, hourly_avg.values)\n",
        "plt.title(\"Average Traffic Volume by Hour of Day\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Average Vehicles\")\n",
        "plt.xticks(range(0,24))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2A â€” Data Preparation for LSTM\n",
        "\n",
        "In this phase, the cleaned dataset is transformed into a format suitable for LSTM modeling.\n",
        "\n",
        "### Steps Performed\n",
        "- Selected a single junction to reduce complexity.\n",
        "- Isolated traffic volume as the prediction target.\n",
        "- Normalized values using Min-Max scaling.\n",
        "- Generated time-series sequences using a 24-hour sliding window.\n",
        "- Split data into training and testing sets while preserving temporal order.\n",
        "\n",
        "### Output\n",
        "- `X_train`, `y_train` for model training\n",
        "- `X_test`, `y_test` for evaluation\n",
        "\n",
        "These sequences capture temporal dependencies required for LSTM learning.\n"
      ],
      "metadata": {
        "id": "6M_EGgzkPOyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 2A: LSTM DATA PREPARATION\n",
        "# =========================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1) Select ONE junction (recommended for first model)\n",
        "junction_id = 1\n",
        "df_junction = df[df['Junction'] == junction_id].copy()\n",
        "\n",
        "print(\"Selected Junction:\", junction_id)\n",
        "print(\"Shape:\", df_junction.shape)\n",
        "\n",
        "# 2) Keep only DateTime and target variable\n",
        "data = df_junction[['DateTime', 'Vehicles']].set_index('DateTime')\n",
        "\n",
        "# 3) Normalize traffic volume (LSTM requirement)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_values = scaler.fit_transform(data[['Vehicles']])\n",
        "\n",
        "# 4) Create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "SEQ_LENGTH = 24  # 24 hours window\n",
        "\n",
        "X, y = create_sequences(scaled_values, SEQ_LENGTH)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# 5) Train-test split (time-aware, no shuffling)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Testing samples:\", X_test.shape[0])\n"
      ],
      "metadata": {
        "id": "tYCq_liNPRQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2B â€” LSTM Model Development and Training\n",
        "\n",
        "In this phase, an LSTM neural network was designed and trained to predict hourly traffic volume.\n",
        "\n",
        "### Model Architecture\n",
        "- Two stacked LSTM layers with 50 units each\n",
        "- Fully connected output layer\n",
        "- Adam optimizer with Mean Squared Error loss\n",
        "\n",
        "### Training Strategy\n",
        "- 30 epochs with early stopping\n",
        "- Learning monitored using validation loss\n",
        "- Overfitting prevented through early stopping\n",
        "\n",
        "### Output\n",
        "- Trained LSTM model\n",
        "- Training and validation loss history\n"
      ],
      "metadata": {
        "id": "c_FY6h1yPWq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 2B: LSTM MODEL DEVELOPMENT\n",
        "# =========================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1) Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    LSTM(50),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# 2) Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "# 3) Model summary (for documentation)\n",
        "model.summary()\n",
        "\n",
        "# 4) Train model\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "cgUJVq4zPYUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2C â€” Model Evaluation (RMSE, MAE) and Prediction Visualization\n",
        "\n",
        "In this phase, the trained LSTM model is evaluated on unseen test data.\n",
        "\n",
        "### Steps Performed\n",
        "- Generated traffic volume predictions on the test set.\n",
        "- Converted predictions back to the original scale using inverse Min-Max scaling.\n",
        "- Evaluated the model using:\n",
        "  - **RMSE (Root Mean Squared Error)**\n",
        "  - **MAE (Mean Absolute Error)**\n",
        "- Visualized model performance by plotting **Actual vs Predicted** traffic volume.\n",
        "\n",
        "### Output\n",
        "- RMSE and MAE values for model performance\n",
        "- Prediction plot showing how closely the model follows real traffic trends\n"
      ],
      "metadata": {
        "id": "-IRhgQl1Qheo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 2C: EVALUATION (RMSE, MAE + PLOTS)\n",
        "# =========================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# 1) Predict on test set\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# 2) Inverse transform back to original \"Vehicles\" scale\n",
        "y_test_inv = scaler.inverse_transform(y_test)\n",
        "y_pred_inv = scaler.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# 3) Compute metrics\n",
        "rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
        "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
        "\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MAE:  {mae:.4f}\")\n",
        "\n",
        "# 4) Plot Actual vs Predicted (first 300 points for readability)\n",
        "N = 300\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(y_test_inv[:N], label=\"Actual\")\n",
        "plt.plot(y_pred_inv[:N], label=\"Predicted\")\n",
        "plt.title(\"Actual vs Predicted Traffic Volume (Test Set)\")\n",
        "plt.xlabel(\"Time Step (Hourly)\")\n",
        "plt.ylabel(\"Vehicles\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ttRf-j4yQkPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving The Model"
      ],
      "metadata": {
        "id": "GYnyheOIRhj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained LSTM model\n",
        "model.save(\"/content/lstm_traffic_model.keras\")\n",
        "\n",
        "print(\"Model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "5y7WaaUcRtQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Persistence\n",
        "\n",
        "The trained LSTM model and data scaler were saved to Google Drive to ensure persistence and reproducibility. This allows the model to be reused for deployment without retraining and protects the project artifacts from runtime disconnections.\n"
      ],
      "metadata": {
        "id": "q2F2adqiSKil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# SAVE PROJECT TO GOOGLE DRIVE\n",
        "# =========================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1) Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2) Create project directory (only once)\n",
        "project_dir = \"/content/drive/MyDrive/LSTM_Traffic_Prediction_Project\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "# 3) Save trained LSTM model\n",
        "model_path = os.path.join(project_dir, \"lstm_traffic_model.keras\")\n",
        "model.save(model_path)\n",
        "\n",
        "print(\"Model saved at:\", model_path)\n"
      ],
      "metadata": {
        "id": "1UMPpv2xSOBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# SAVE SCALER\n",
        "# =========================================\n",
        "\n",
        "import joblib\n",
        "\n",
        "scaler_path = os.path.join(project_dir, \"minmax_scaler.save\")\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "print(\"Scaler saved at:\", scaler_path)\n"
      ],
      "metadata": {
        "id": "y_zYcC30SRg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3 â€” Step 1: Environment Reset and Artifact Verification\n",
        "\n",
        "Before deployment, we reset any running processes from previous attempts and verify that the trained artifacts are safely stored in Google Drive.\n",
        "\n",
        "### Actions\n",
        "- Stopped any previously running Streamlit/tunneling processes.\n",
        "- Mounted Google Drive.\n",
        "- Verified presence of:\n",
        "  - `lstm_traffic_model.keras` (trained LSTM model)\n",
        "  - `minmax_scaler.save` (MinMaxScaler used during training)\n",
        "- Listed project folder contents to confirm correct file paths.\n",
        "\n",
        "### Output\n",
        "Confirmed model and scaler files exist and are ready for deployment.\n"
      ],
      "metadata": {
        "id": "IctOj0Ejat8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 3 (STEP 1): CLEAN RESTART + VERIFY FILES\n",
        "# =========================================\n",
        "\n",
        "# 1) Stop anything left running (Streamlit/ngrok/etc.)\n",
        "!pkill -f streamlit || true\n",
        "!pkill -f ngrok || true\n",
        "!pkill -f cloudflared || true\n",
        "\n",
        "# 2) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3) Verify artifacts exist\n",
        "import os\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/LSTM_Traffic_Prediction_Project\"\n",
        "model_path = os.path.join(project_dir, \"lstm_traffic_model.keras\")\n",
        "scaler_path = os.path.join(project_dir, \"minmax_scaler.save\")\n",
        "\n",
        "print(\"Project dir:\", project_dir)\n",
        "print(\"Model exists:\", os.path.exists(model_path), \"|\", model_path)\n",
        "print(\"Scaler exists:\", os.path.exists(scaler_path), \"|\", scaler_path)\n",
        "\n",
        "# 4) List folder contents\n",
        "print(\"\\nFolder contents:\")\n",
        "print(os.listdir(project_dir))\n"
      ],
      "metadata": {
        "id": "KZLwgei2avj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3 â€” Step 2: Web Application Development\n",
        "\n",
        "A Streamlit-based web application was created to deploy the trained LSTM traffic prediction model.\n",
        "\n",
        "### Features\n",
        "- Loads the trained LSTM model and scaler from Google Drive\n",
        "- Accepts traffic volume values for the previous 24 hours\n",
        "- Predicts traffic volume for the next hour\n",
        "- Displays results in a user-friendly interface\n",
        "\n",
        "### Output\n",
        "A complete `app.py` file ready for deployment.\n"
      ],
      "metadata": {
        "id": "0KcTlW_4bGhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 3 (STEP 2): CREATE STREAMLIT APP\n",
        "# =========================================\n",
        "\n",
        "app_code = r'''\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# ------------------------------\n",
        "# Page Configuration\n",
        "# ------------------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"Traffic Volume Prediction (LSTM)\",\n",
        "    page_icon=\"ðŸš¦\",\n",
        "    layout=\"centered\"\n",
        ")\n",
        "\n",
        "st.title(\"ðŸš¦ Traffic Volume Prediction System\")\n",
        "st.write(\n",
        "    \"This system predicts **next-hour traffic volume** using a trained \"\n",
        "    \"**LSTM model** based on the I-94 Traffic Dataset.\"\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# Load Model & Scaler\n",
        "# ------------------------------\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/LSTM_Traffic_Prediction_Project\"\n",
        "MODEL_PATH = os.path.join(PROJECT_DIR, \"lstm_traffic_model.keras\")\n",
        "SCALER_PATH = os.path.join(PROJECT_DIR, \"minmax_scaler.save\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_artifacts():\n",
        "    model = tf.keras.models.load_model(MODEL_PATH)\n",
        "    scaler = joblib.load(SCALER_PATH)\n",
        "    return model, scaler\n",
        "\n",
        "model, scaler = load_artifacts()\n",
        "\n",
        "# ------------------------------\n",
        "# Input Section\n",
        "# ------------------------------\n",
        "st.subheader(\"Input: Last 24 Hourly Traffic Volumes\")\n",
        "\n",
        "st.info(\"Enter traffic volume values for the previous 24 hours.\")\n",
        "\n",
        "inputs = []\n",
        "cols = st.columns(4)\n",
        "\n",
        "for i in range(24):\n",
        "    with cols[i % 4]:\n",
        "        value = st.number_input(\n",
        "            f\"Hour {i+1}\",\n",
        "            min_value=0.0,\n",
        "            value=50.0,\n",
        "            step=1.0\n",
        "        )\n",
        "        inputs.append(value)\n",
        "\n",
        "# ------------------------------\n",
        "# Prediction\n",
        "# ------------------------------\n",
        "if st.button(\"Predict Next Hour Traffic\"):\n",
        "    data = np.array(inputs).reshape(-1, 1)\n",
        "\n",
        "    data_scaled = scaler.transform(data)\n",
        "    X_input = data_scaled.reshape(1, 24, 1)\n",
        "\n",
        "    prediction_scaled = model.predict(X_input)\n",
        "    prediction = scaler.inverse_transform(prediction_scaled)\n",
        "\n",
        "    st.success(\n",
        "        f\"âœ… **Predicted Traffic Volume (Next Hour): \"\n",
        "        f\"{prediction[0][0]:.2f} vehicles**\"\n",
        "    )\n",
        "\n",
        "# ------------------------------\n",
        "# Footer\n",
        "# ------------------------------\n",
        "st.caption(\n",
        "    \"Model: LSTM | Window Size: 24 Hours | \"\n",
        "    \"Dataset: I-94 Traffic Volume\"\n",
        ")\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"âœ… app.py created successfully.\")\n"
      ],
      "metadata": {
        "id": "FAOONtJja0CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3 â€” Step 3: Launch Streamlit Server\n",
        "\n",
        "The Streamlit application was launched inside the Colab runtime on port 8501.\n",
        "\n",
        "### Actions\n",
        "- Installed deployment dependencies (Streamlit + joblib).\n",
        "- Started the Streamlit server in the background.\n",
        "- Verified successful startup by checking the server logs.\n",
        "\n",
        "### Output\n",
        "A running Streamlit server listening on port `8501`.\n"
      ],
      "metadata": {
        "id": "11Qx9xxYbZzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 3 (STEP 3): RUN STREAMLIT LOCALLY\n",
        "# =========================================\n",
        "\n",
        "!pip -q install streamlit joblib\n",
        "\n",
        "# Stop any previous Streamlit process (safe)\n",
        "!pkill -f streamlit || true\n",
        "\n",
        "# Run Streamlit in background and log output to a file\n",
        "!streamlit run app.py --server.port 8501 --server.headless true > streamlit.log 2>&1 &\n",
        "\n",
        "# Quick check: show last lines of log\n",
        "!tail -n 20 streamlit.log\n"
      ],
      "metadata": {
        "id": "DrMNfGB8bMdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3 â€” Step 4A: Install Cloudflare Tunnel (cloudflared)\n",
        "\n",
        "Cloudflare Tunnel was installed to expose the Streamlit app publicly from the Colab runtime.\n",
        "All shell commands are executed using `!` in Colab.\n"
      ],
      "metadata": {
        "id": "Qw3aeTRIgAwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 3 (STEP 4A): INSTALL CLOUDFLARED (FIXED)\n",
        "# =========================================\n",
        "\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "!mv cloudflared /usr/local/bin/cloudflared\n",
        "\n",
        "!cloudflared --version\n"
      ],
      "metadata": {
        "id": "14cMijwUctow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit || true\n",
        "!pkill -f cloudflared || true\n",
        "!lsof -i :8501 || true\n"
      ],
      "metadata": {
        "id": "Ng5nNpwSjXgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Streamlit in background and log output\n",
        "!streamlit run app.py --server.port 8501 --server.headless true --server.address 0.0.0.0 > streamlit.log 2>&1 &\n",
        "\n",
        "# Show logs (look for \"Running on...\" and no errors)\n",
        "!tail -n 40 streamlit.log\n",
        "\n",
        "# Confirm something is listening on port 8501\n",
        "!lsof -i :8501 | head -n 5\n"
      ],
      "metadata": {
        "id": "s0qGd0bBjbga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3 â€” Step 4B: Generate Public URL\n",
        "\n",
        "A Cloudflare Tunnel is started to expose the Streamlit application running locally on port 8501.\n",
        "The output provides a `trycloudflare.com` URL which can be used to access the system in a browser for live demo.\n"
      ],
      "metadata": {
        "id": "BN85kHRwhRix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# PHASE 3 (STEP 4B): START PUBLIC TUNNEL\n",
        "# =========================================\n",
        "\n",
        "!cloudflared tunnel --url http://localhost:8501\n"
      ],
      "metadata": {
        "id": "CWK34zU6gkUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"app.py\")\n"
      ],
      "metadata": {
        "id": "2vW_ATKtxs3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}